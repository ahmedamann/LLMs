{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "381f1607-2059-44c3-b2f1-3a7f9b71ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e34a1a64-73c1-43e3-a7b6-d6cc2b0f45e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Website:\n",
    "    def __init__(self, url, *, extend_search=False):\n",
    "        self.url = url\n",
    "        self.text = \"\"\n",
    "        text, links = self._get_contents(url)\n",
    "        self.text = text\n",
    "        if links:\n",
    "            self.links = self._parse_links(links)\n",
    "\n",
    "        if extend_search:\n",
    "            for link in self.links:\n",
    "                text, _ = self._get_contents(link)\n",
    "                self.text += \"\".join(text)\n",
    "                print(\"Text from a link was added\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_contents(link):\n",
    "        response = requests.get(link) # get response\n",
    "        body = response.content # get the content of the reponse\n",
    "        soup = BeautifulSoup(body, 'html.parser') # parse the content with html parser\n",
    "        \n",
    "        if soup.body:\n",
    "            for irr in soup.body(['script', 'style', 'img', 'input', 'iframe', 'button', 'nav', 'footer']):\n",
    "                irr.decompose()\n",
    "            text = \"\\n\".join(\n",
    "                [el.get_text(strip=True) for el in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "            ])\n",
    "        else:\n",
    "            text = \"\"\n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        \n",
    "        return text, links\n",
    "\n",
    "    def _parse_links(self, links):\n",
    "        parsed_links = []\n",
    "        for link in links:\n",
    "            if link:\n",
    "                if link.startswith(\"https://\") or link.startswith(\"www\"):\n",
    "                    parsed_links.append(link)\n",
    "                elif not link.startswith(\"/\"):\n",
    "                    parsed_links.append(self.url+link)\n",
    "                else:\n",
    "                    parsed_links.append(self.url.rstrip('/')+link)\n",
    "        return parsed_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b6b54249-5b74-41e1-8fe1-cbf7b837810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_system_prompt = link_system_prompt = \"\"\"\n",
    "You are provided with a list of links on a webpage.\n",
    "You must decide which of these links would be the most relevant to include in a brochure about the company.\n",
    "Do not include Terms of Service, privacy, or email links.\n",
    "Respond in valid JSON format exactly as shown in the example below.\n",
    "Make sure that all keys and string values (including URLs) are enclosed in double quotes.\n",
    "\n",
    "Example:\n",
    "{{\n",
    "    \"links\": [\n",
    "        {{ \"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\" }},\n",
    "        {{ \"type\": \"careers page\", \"url\": \"https://full.url/goes/here/careers\" }}\n",
    "    ]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "link_user_prompt = \"\"\"\n",
    "    Here is the list of links on the website:\n",
    "    {links}.\n",
    "    \n",
    "    Please decide which of these are relevant web links for a brochure about the company:\n",
    "\"\"\"\n",
    "\n",
    "def get_links(url):\n",
    "    website = Website(url)\n",
    "    return {\"links\": \"\\n\".join(website.links)}\n",
    "\n",
    "llm = OllamaLLM(model=\"mistral\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", link_system_prompt),\n",
    "        (\"human\", link_user_prompt)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0044d66c-0551-467a-8cac-35d81a72e4bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model 'mistral' not found (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JsonOutputParser\n\u001b[1;32m      3\u001b[0m chain \u001b[38;5;241m=\u001b[39m RunnableLambda(get_links) \u001b[38;5;241m|\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm \u001b[38;5;241m|\u001b[39m JsonOutputParser()\n\u001b[0;32m----> 4\u001b[0m rel_links \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.langchain.com/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:387\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    384\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    385\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 387\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    388\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    389\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    390\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    391\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    392\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    393\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    394\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    396\u001b[0m         )\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    399\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:760\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    754\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    758\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    759\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:963\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    950\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    951\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    961\u001b[0m         )\n\u001b[1;32m    962\u001b[0m     ]\n\u001b[0;32m--> 963\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[1;32m    964\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    965\u001b[0m     )\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:784\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    776\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    781\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 784\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    785\u001b[0m                 prompts,\n\u001b[1;32m    786\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    787\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[1;32m    788\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    789\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    790\u001b[0m             )\n\u001b[1;32m    791\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    792\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    793\u001b[0m         )\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/langchain_ollama/llms.py:288\u001b[0m, in \u001b[0;36mOllamaLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 288\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[1;32m    289\u001b[0m         prompt,\n\u001b[1;32m    290\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    291\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m    292\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    294\u001b[0m     )\n\u001b[1;32m    295\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/langchain_ollama/llms.py:256\u001b[0m, in \u001b[0;36mOllamaLLM._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    249\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    254\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[1;32m    255\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    258\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[1;32m    259\u001b[0m                 text\u001b[38;5;241m=\u001b[39mstream_resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    260\u001b[0m                 generation_info\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    261\u001b[0m                     \u001b[38;5;28mdict\u001b[39m(stream_resp) \u001b[38;5;28;01mif\u001b[39;00m stream_resp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    262\u001b[0m                 ),\n\u001b[1;32m    263\u001b[0m             )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/langchain_ollama/llms.py:211\u001b[0m, in \u001b[0;36mOllamaLLM._create_generate_stream\u001b[0;34m(self, prompt, stop, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    208\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    210\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_params(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    213\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/ollama/_client.py:168\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    167\u001b[0m   e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 168\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[1;32m    171\u001b[0m   part \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "\u001b[0;31mResponseError\u001b[0m: model 'mistral' not found (status code: 404)"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = RunnableLambda(get_links) | prompt | llm | JsonOutputParser()\n",
    "rel_links = chain.invoke('https://www.langchain.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fba9ebd1-a426-43b5-be22-b0794faf11b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_for_all_links(relevant_links):\n",
    "    text = \"\"\n",
    "    for link in relevant_links[\"links\"]:\n",
    "        text += f\"Page: {link['type']}\"\n",
    "        text += Website(link['url']).text\n",
    "    return text\n",
    "    \n",
    "text = get_text_for_all_links(rel_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fbdf021d-4771-4109-add0-5701960cb7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_brochure_system_template = \"\"\"\n",
    "You are an expert at analyzing the contents of several pages from a company website.\n",
    "Using the provided content, create a short, attractive brochure for the company.\n",
    "Include details that capture the company's essence solely based on the context provided.\n",
    "Return your response in Markdown format, using appropriate headings, bullet points, and emphasis.\\\n",
    "\n",
    "Context:\n",
    "\n",
    "{content}\n",
    "\"\"\"\n",
    "\n",
    "make_brochure_user_template = \"\"\"\n",
    "Create a short brochure for {company_name} company:\n",
    "\"\"\"\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", make_brochure_system_template),\n",
    "        (\"human\", make_brochure_user_template)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cf6306d4-43d4-4333-83c7-f6d3dbfe523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = final_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "91bc391a-adc1-484b-bac0-88f000d849e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "brochure = final_chain.invoke({\"content\":text, \"company_name\": \"LangChain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "37013243-d520-4f4d-9d9e-6f08c0b890aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is a potential brochure design for LangChain:\n",
       "\n",
       "**[Cover Page]**\n",
       "\n",
       "[Image of a futuristic AI landscape]\n",
       "\n",
       "Welcome to LangChain\n",
       "Accelerating GenAI Development\n",
       "\n",
       "**[Page 1: Introduction]**\n",
       "\n",
       "At LangChain, we believe that the future of artificial intelligence is here. Our mission is to empower developers and organizations to build, deploy, and maintain reliable and scalable GenAI applications.\n",
       "\n",
       "With LangChain's cutting-edge products, you can:\n",
       "\n",
       "* Boost your development speed by up to 10x\n",
       "* Improve model accuracy and performance\n",
       "* Reduce engineering intervention by up to 90%\n",
       "* Enhance collaboration with our open-source community\n",
       "\n",
       "**[Page 2: Products]**\n",
       "\n",
       "Our suite of products includes:\n",
       "\n",
       "* **LangChain**: Our flagship product, powering the world's top GenAI applications.\n",
       "* **LangGraph**: A powerful graph-based framework for building scalable AI models.\n",
       "* **LangSmith**: An expert system that accelerates model training and iteration.\n",
       "* **LangGrapht**: A flexible graph processing engine for complex data pipelines.\n",
       "\n",
       "**[Page 3: Customer Success Stories]**\n",
       "\n",
       "Don't just take our word for it. Our customers have seen significant improvements in development speed, accuracy, and reliability:\n",
       "\n",
       "* Klarna's AI assistant redefined customer support at scale for 85 million active users\n",
       "* Rakuten Group builds with LangChain and LangSmith to deliver premium products for its business clients\n",
       "* Replit redefined their AI agent workflows with LangGraph and LangSmith\n",
       "\n",
       "**[Page 4: Benefits]**\n",
       "\n",
       "By using LangChain, you'll enjoy:\n",
       "\n",
       "* **Faster Development**: Build and deploy GenAI applications up to 10x faster.\n",
       "* **Improved Accuracy**: Boost model accuracy and performance by up to 50%.\n",
       "* **Reduced Engineering Intervention**: Automate tedious tasks and focus on high-value work.\n",
       "* **Enhanced Collaboration**: Join our open-source community and tap into expert knowledge.\n",
       "\n",
       "**[Back Cover]**\n",
       "\n",
       "Ready to start shipping reliable GenAI apps faster? Try LangChain today!\n",
       "\n",
       "[Contact Information: Email, Phone Number, Website]\n",
       "\n",
       "Let's build the future of AI together."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(brochure))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlpenv)",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
